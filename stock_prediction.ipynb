{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXcsJ6RFvzzc",
        "outputId": "b79e5443-538b-4d30-9992-6f2d2e83b24d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "True\n",
            "True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "<ipython-input-4-e49ce3173b4b>:108: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_data['sentiment'].fillna(0, inplace=True)\n",
            "<ipython-input-4-e49ce3173b4b>:109: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_data['Domain'].fillna(\"No Domain\", inplace=True)\n",
            "<ipython-input-4-e49ce3173b4b>:110: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  merged_data['Impact_Factor'].fillna(\"No Impact Factor\", inplace=True)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of features: (62, 76)\n",
            "Shape of target: (62,)\n",
            "x_seq shape after creating sequences: (42, 20, 76)\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - loss: 0.3855 - val_loss: 0.0356\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0893 - val_loss: 0.0339\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.1763 - val_loss: 0.0105\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0849 - val_loss: 0.0447\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0748 - val_loss: 0.0538\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0915 - val_loss: 0.0340\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0659 - val_loss: 0.0095\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0697 - val_loss: 0.0187\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0640 - val_loss: 0.0217\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0541 - val_loss: 0.0090\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0357 - val_loss: 0.0163\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0477 - val_loss: 0.0135\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - loss: 0.0620 - val_loss: 0.0194\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0247 - val_loss: 0.0207\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0133 - val_loss: 0.0338\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459ms/step\n",
            "First 5 Predictions: [[1918.7821]\n",
            " [1920.4052]\n",
            " [1921.0874]\n",
            " [1922.1799]\n",
            " [1922.4551]\n",
            " [1919.6416]\n",
            " [1913.4353]\n",
            " [1904.4762]\n",
            " [1894.6093]]\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 508ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "(1, 20, 76)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation_df columns: Index(['date', 'Predicted_Close', 'actual_close'], dtype='object')\n",
            "daily_news columns: Index(['date', 'sentiment', 'Domain', 'Impact_Factor'], dtype='object')\n",
            "        date  ...                                      Impact_Factor\n",
            "0 2024-12-24  ...                             None, Trade Agreements\n",
            "1 2024-12-25  ...                                    None, Sanctions\n",
            "2 2024-12-26  ...                             None, Trade Agreements\n",
            "3 2024-12-27  ...  Acquisitions, Mergers, New Regulations, Merger...\n",
            "4 2024-12-28  ...                            None, Market Volatility\n",
            "\n",
            "[5 rows x 4 columns]\n",
            "         date  Predicted Close Price  Actual Close Price  ...  Prediction Domain Impact Factor  MAPE (%)\n",
            "0  2025-01-02            1888.265869         1957.650024  ...                NaN           NaN  3.544257\n",
            "1  2025-01-03            1881.885864         1938.300049  ...                NaN           NaN  2.910498\n",
            "2  2025-01-06            1865.317139         1937.849976  ...                NaN           NaN  3.742954\n",
            "3  2025-01-07            1860.770508         1932.550049  ...                NaN           NaN  3.714240\n",
            "4  2025-01-08            1856.693359         1933.150024  ...                NaN           NaN  3.955030\n",
            "5  2025-01-09            1853.103638         1917.800049  ...                NaN           NaN  3.373470\n",
            "6  2025-01-10            1849.900513         1966.699951  ...                NaN           NaN  5.938854\n",
            "7  2025-01-13            1842.276611         1961.800049  ...                NaN           NaN  6.092539\n",
            "8  2025-01-14            1840.260742         1939.300049  ...                NaN           NaN  5.106961\n",
            "9  2025-01-15            1838.327637         1949.800049  ...                NaN           NaN  5.717120\n",
            "10 2025-01-16            1836.537354         1926.199951  ...                NaN           NaN  4.654896\n",
            "\n",
            "[11 rows x 7 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === Step 1: Import Libraries ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import yfinance as yf\n",
        "\n",
        "from transformers import pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# === Step 2: Load Data ===\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "stock_data_path = \"/content/drive/MyDrive/stock_prediction/INFY.csv\"\n",
        "news_data_path = \"/content/drive/MyDrive/stock_prediction/news_data.csv\"\n",
        "\n",
        "import os\n",
        "print(os.path.exists(stock_data_path))\n",
        "print(os.path.exists(news_data_path))\n",
        "\n",
        "stock_data = pd.read_csv(stock_data_path)\n",
        "news_data = pd.read_csv(news_data_path)\n",
        "\n",
        "#print(stock_data.head())\n",
        "#print(news_data.head())\n",
        "\n",
        "stock_data['date'] = pd.to_datetime(stock_data['date'], errors='coerce')\n",
        "stock_data['date'] = stock_data['date'].dt.tz_localize(None)\n",
        "\n",
        "news_data.rename(columns={'published_at': 'date'}, inplace=True)\n",
        "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce').dt.date\n",
        "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
        "\n",
        "#print(\"stock_data:\", stock_data.head())\n",
        "#print(\"news_data:\", news_data.head())\n",
        "stock_data = stock_data.sort_values(by=\"date\")\n",
        "news_data = news_data.sort_values(by=\"date\")\n",
        "\n",
        "stock_data = stock_data[(stock_data['date'] >= \"2024-10-01\") & (stock_data['date'] <= \"2024-12-31\")]\n",
        "news_data = news_data[(news_data['date'] >= \"2024-10-01\") & (news_data['date'] <= \"2024-12-31\")]\n",
        "\n",
        "# === Step 3: Preprocess News Data ===\n",
        "news_data['title'] = news_data['title'].str.lower()\n",
        "news_data['description'] = news_data['description'].str.lower()\n",
        "\n",
        "#print(stock_data.head())\n",
        "#print(news_data.head())\n",
        "domains = [\n",
        "    \"Economy\", \"Technology\", \"Finance\", \"Politics\", \"Healthcare\", \"Energy\", \"Automotive\", \"Retail\", \"Real Estate\",\n",
        "    \"Entertainment\", \"Education\", \"Agriculture\", \"Environment\", \"Infrastructure\", \"Telecom\", \"Defense\",\n",
        "    \"Social Media\", \"Travel & Tourism\", \"Cryptocurrency\", \"Consumer Goods\"\n",
        "]\n",
        "\n",
        "impact_factors = [\n",
        "    \"Positive Market Sentiment\", \"Negative Market Sentiment\", \"Regulatory Changes\", \"Policy Announcements\",\n",
        "    \"Trade Agreements\", \"Inflation Data\", \"Interest Rate Changes\", \"Company Earnings Reports\", \"Product Launches\",\n",
        "    \"Layoffs\", \"Acquisitions\", \"Mergers\", \"Partnerships\", \"Legal Disputes\", \"Scandals\", \"Technological Breakthroughs\",\n",
        "    \"Cybersecurity Breaches\", \"Climate Reports\", \"Natural Disasters\", \"Global Conflicts\", \"Sanctions\",\n",
        "    \"IPO Announcements\", \"Stock Buybacks\", \"Dividend Announcements\", \"Industry Growth Reports\", \"Bankruptcies\",\n",
        "    \"Start-up Funding\", \"CEO Changes\", \"Pandemic-Related News\", \"Labor Strikes\", \"Supply Chain Disruptions\",\n",
        "    \"Oil Price Fluctuations\", \"Commodity Price Fluctuations\", \"Tax Reforms\", \"Trade War Updates\", \"Retail Sales Data\",\n",
        "    \"New Regulations\", \"Consumer Behavior Changes\", \"Market Volatility\", \"Export/Import Data\", \"Currency Exchange Rates\",\n",
        "    \"Technology Adoption Trends\", \"Market Penetration Strategies\", \"Interest from Institutional Investors\",\n",
        "    \"Analyst Upgrades/Downgrades\", \"Lawsuits Against Competitors\", \"International Market Trends\",\n",
        "    \"Political Instability\", \"Demand-Supply Trends\", \"Ethical Issues\"\n",
        "]\n",
        "\n",
        "def assign_domain_and_impact_factor(text):\n",
        "    text = text.lower()\n",
        "    matched_domains = [domain for domain in domains if domain.lower() in text]\n",
        "    matched_factors = [factor for factor in impact_factors if factor.lower() in text]\n",
        "    return \", \".join(matched_domains) if matched_domains else \"Other\", \", \".join(matched_factors) if matched_factors else \"None\"\n",
        "\n",
        "news_data[['Domain', 'Impact_Factor']] = news_data.apply(\n",
        "    lambda row: pd.Series(assign_domain_and_impact_factor(row['title'] + \" \" + row['description'])), axis=1\n",
        ")\n",
        "#print(news_data.head())\n",
        "# === Step 4: sentiment analysis ===\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    if pd.isna(text) or text.strip() == \"\":\n",
        "        return 0\n",
        "    result = sentiment_analyzer(text)[0]\n",
        "    return result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
        "\n",
        "news_data['sentiment'] = news_data.apply(\n",
        "    lambda row: analyze_sentiment(row['title'] + \" \" + row['description']), axis=1\n",
        ")\n",
        "\n",
        "\n",
        "news_data['date'] = pd.to_datetime(news_data['date'], errors='coerce')\n",
        "stock_data['date'] = pd.to_datetime(stock_data['date'], errors='coerce')\n",
        "\n",
        "daily_news = news_data.groupby(pd.Grouper(key='date', freq='D')).agg({\n",
        "    'sentiment': 'mean',\n",
        "    'Domain': lambda x: ', '.join(set(x.dropna())),\n",
        "    'Impact_Factor': lambda x: ', '.join(set(x.dropna()))\n",
        "}).reset_index()\n",
        "\n",
        "#print(daily_news.head())\n",
        "merged_data = pd.merge(stock_data, daily_news, on='date', how='left')\n",
        "\n",
        "merged_data['sentiment'].fillna(0, inplace=True)\n",
        "merged_data['Domain'].fillna(\"No Domain\", inplace=True)\n",
        "merged_data['Impact_Factor'].fillna(\"No Impact Factor\", inplace=True)\n",
        "\n",
        "for domain in domains:\n",
        "    merged_data[f'Domain_{domain}_Count'] = merged_data['Domain'].str.contains(domain, na=False).astype(int)\n",
        "\n",
        "for factor in impact_factors:\n",
        "    merged_data[f'Factor_{factor}_Count'] = merged_data['Impact_Factor'].str.contains(factor, na=False).astype(int)\n",
        "\n",
        "merged_data.dropna(inplace=True)\n",
        "\n",
        "merged_data.drop(columns=['Domain', 'Impact_Factor'], inplace=True)\n",
        "\n",
        "#print(\"Merged Data:\", merged_data.head())\n",
        "#print(\"Total Rows:\", len(merged_data))\n",
        "\n",
        "\n",
        "# === Step 5: LSTM ===\n",
        "features = merged_data.drop(columns=['date', 'close']).values\n",
        "target = merged_data['close'].values\n",
        "\n",
        "print(\"Shape of features:\", features.shape)\n",
        "print(\"Shape of target:\", target.shape)\n",
        "\n",
        "scaler_y = MinMaxScaler()\n",
        "target_scaled = scaler_y.fit_transform(target.reshape(-1, 1)).flatten()\n",
        "\n",
        "scaler_X = MinMaxScaler()\n",
        "features_scaled = scaler_X.fit_transform(features)\n",
        "\n",
        "def create_sequences(data, labels, sequence_length=20):\n",
        "    x_seq, y_seq = [], []\n",
        "    for i in range(len(data) - sequence_length):\n",
        "        x_seq.append(data[i:i+sequence_length])\n",
        "        y_seq.append(labels[i+sequence_length])\n",
        "    return np.array(x_seq), np.array(y_seq)\n",
        "\n",
        "x_seq, y_seq = create_sequences(features_scaled, target_scaled)\n",
        "\n",
        "print(f\"x_seq shape after creating sequences: {x_seq.shape}\")\n",
        "\n",
        "train_size = int(len(x_seq) * 0.8)\n",
        "x_train, x_test = x_seq[:train_size], x_seq[train_size:]\n",
        "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    LSTM(32),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=50, batch_size=8,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "y_pred_scaled = model.predict(x_test)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
        "\n",
        "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(\"First 5 Predictions:\", y_pred)\n",
        "#print(\"First 5 Actual Values:\", y_test_original[:5].flatten())\n",
        "\n",
        "\n",
        "# === Step 6: Prediction ===\n",
        "predictions = []\n",
        "x_last = x_test[-1].reshape(1, 20, 76)\n",
        "\n",
        "for _ in range(17):\n",
        "\n",
        "\n",
        "    print(x_last.shape)\n",
        "\n",
        "    pred_price_scaled = model.predict(x_last)[0, 0]\n",
        "    predictions.append(pred_price_scaled)\n",
        "\n",
        "    new_row = np.zeros((1, 20, 76))\n",
        "    new_row[0, -1, :] = features_scaled[-1]\n",
        "    new_row[0, :-1, :] = x_last[0, 1:, :]\n",
        "    new_row[0, -1, -1] = pred_price_scaled\n",
        "\n",
        "    x_last = new_row\n",
        "\n",
        "predictions_original = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
        "\n",
        "last_date = stock_data['date'].max()\n",
        "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=17, freq='D')\n",
        "\n",
        "output_df = pd.DataFrame({\"date\": future_dates, \"Predicted_Close\": predictions_original})\n",
        "output_df.to_csv(\"output.csv\", index=False)\n",
        "\n",
        "#print(output_df)\n",
        "\n",
        "actual_stock_df = yf.download(\"INFY.BO\", start=\"2025-01-02\", end=\"2025-01-17\")[['Close']].reset_index()\n",
        "actual_stock_df.rename(columns={'Date': 'date', 'Close': 'actual_close'}, inplace=True)\n",
        "\n",
        "if isinstance(actual_stock_df.columns, pd.MultiIndex):\n",
        "    actual_stock_df.columns = actual_stock_df.columns.droplevel(1)\n",
        "\n",
        "\n",
        "evaluation_df = pd.merge(output_df, actual_stock_df, on='date', how='inner')\n",
        "\n",
        "#print(\"Merged Data:\",evaluation_df)\n",
        "# Compute MAPE\n",
        "mape = mean_absolute_percentage_error(evaluation_df['actual_close'], evaluation_df['Predicted_Close'])\n",
        "#print(f\"Mean Absolute Percentage Error (MAPE): {mape * 100:.2f}%\")\n",
        "\n",
        "\n",
        "#print(\"Predicted Prices:\",output_df)\n",
        "#print(\"Actual Stock Prices:\",actual_stock_df)\n",
        "\n",
        "print(\"evaluation_df columns:\", evaluation_df.columns)\n",
        "print(\"daily_news columns:\", daily_news.columns)\n",
        "#print(daily_news.columns)\n",
        "print(daily_news.head())\n",
        "\n",
        "#print(evaluation_df.head())\n",
        "\n",
        "final_output_df = pd.merge(evaluation_df, daily_news, on='date', how='left')\n",
        "\n",
        "final_output_df['MAPE (%)'] = abs((final_output_df['actual_close'] - final_output_df['Predicted_Close']) / final_output_df['actual_close']) * 100\n",
        "\n",
        "final_output_df.rename(columns={\n",
        "    'Predicted_Close': 'Predicted Close Price',\n",
        "    'actual_close': 'Actual Close Price',\n",
        "    'sentiment': 'Sentiment Score',\n",
        "    'Domain': 'Prediction Domain',\n",
        "    'Impact_Factor': 'Impact Factor'\n",
        "}, inplace=True)\n",
        "\n",
        "# Save to CSV\n",
        "final_output_df.to_csv(\"/content/drive/MyDrive/stock_prediction/final_ouput.csv\", index=False)\n",
        "\n",
        "# Display the final dataframe\n",
        "print(final_output_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Uy3jNDj-2VX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}